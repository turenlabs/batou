\documentclass[11pt,twocolumn]{article}

% --- Packages ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[margin=0.85in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{array}
\usepackage{tabularx}
\usepackage{float}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{shapes,arrows.meta,positioning,calc,fit,backgrounds}

\hypersetup{
  colorlinks=true,
  linkcolor=blue!70!black,
  citecolor=blue!70!black,
  urlcolor=blue!70!black,
}

% --- Code listing style ---
\definecolor{codebg}{rgb}{0.96,0.96,0.96}
\definecolor{codegreen}{rgb}{0.0,0.5,0.0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.5,0.0,0.5}

\lstdefinestyle{default}{
  backgroundcolor=\color{codebg},
  basicstyle=\ttfamily\scriptsize,
  breaklines=true,
  captionpos=b,
  commentstyle=\color{codegreen},
  keywordstyle=\color{codepurple}\bfseries,
  stringstyle=\color{red!60!black},
  numbers=left,
  numberstyle=\tiny\color{codegray},
  numbersep=5pt,
  frame=single,
  framerule=0.4pt,
  xleftmargin=1.5em,
  framexleftmargin=1.5em,
  tabsize=2,
  showstringspaces=false,
}
\lstset{style=default}

% --- Title ---
\title{%
  \textbf{ProductSecBench: A Multi-Phase Security Benchmark\\for LLM Code Generation}
}

\author{%
  Turen Research\\
  \texttt{hello@turen.io}\\
  \url{https://turen.io}
}

\date{February 2026}

\begin{document}
\maketitle

% ============================================================
\begin{abstract}
Large language models generate code with known vulnerability patterns, yet existing
security benchmarks evaluate only a single generation pass with binary scoring.
We present ProductSecBench (PSB), a multi-phase benchmark for evaluating the
security of LLM-generated code in realistic product development scenarios.
PSB introduces three innovations over prior work: (1)~a four-phase evaluation
protocol that measures baseline vulnerability rates, the effect of security
priming, scanner-augmented generation, and self-correction from structured
hints; (2)~severity-weighted composite scoring aligned with product security
risk assessment; and (3)~the Self-Correction Rate (SCR), a new metric
quantifying an LLM's ability to fix vulnerabilities given actionable feedback.

The benchmark comprises 43 prompts across 8 OWASP Top~10 categories and
8~programming languages, with gold-standard vulnerable and secure code pairs.
Using GTSS (Generation-Time Security Scanning) as the reference scanner,
preliminary evaluation on the gold-standard corpus shows 98\% detection recall
and a 74\% hard-block rate for critical vulnerabilities, establishing a
baseline for cross-model evaluation. PSB is implemented as a zero-dependency
Go test suite, enabling any researcher to reproduce results with
\texttt{go test ./bench/...}.
\end{abstract}

% ============================================================
\section{Introduction}

AI coding assistants---Claude Code, GitHub Copilot, Cursor, and others---have
fundamentally changed software development. Developers delegate entire features
to LLMs, reviewing generated code only cursorily before accepting it. This
acceleration creates a security gap: empirical studies show LLMs produce
vulnerable code at alarming rates, including 88\% failure rates for log
injection and 86\% for cross-site scripting~\cite{pearce2022}.

The problem is well-documented. Pearce et al.~\cite{pearce2022} found that
GitHub Copilot generates insecure code in approximately 40\% of security-relevant
scenarios. Sandoval et al.~\cite{sandoval2023} demonstrated that developers
using AI assistants produce code with more security vulnerabilities than those
coding manually, despite writing more code overall. These findings have
motivated a growing body of security benchmarks for LLM code
generation~\cite{cyberseceval,codelmssec,securityeval,cweval,baxbench,safegenbench}.

However, existing benchmarks share a critical limitation: they evaluate a single
generation pass. The LLM receives a prompt, produces code, and the code is
scanned---once. This ``generate-and-judge'' paradigm misses the most important
dimension of LLM-assisted development: the \emph{feedback loop}. In practice,
developers iterate with their AI assistant. Security scanners flag issues.
The LLM receives hints and rewrites. The question is not just ``does the LLM
generate vulnerable code?'' but ``can it fix the vulnerability when told about it?''

We present ProductSecBench (PSB), a benchmark designed to answer this question.
PSB evaluates LLMs through four sequential phases:

\begin{enumerate}[nosep]
  \item \textbf{Baseline}: vanilla generation with no security guidance.
  \item \textbf{Security-primed}: generation with ``write secure code'' in the
        system prompt.
  \item \textbf{Scanner-augmented}: generation with real-time scanner feedback
        and iterative correction.
  \item \textbf{Self-correction}: given a known-vulnerable sample and a
        structured hint, the LLM attempts a targeted fix.
\end{enumerate}

This multi-phase design captures the full developer workflow and isolates
the security uplift provided by each intervention. PSB also introduces
severity-weighted scoring (not all vulnerabilities are equally dangerous)
and the Self-Correction Rate (SCR), a new metric that directly measures
the value of the scanner feedback loop.

Our contributions are:
\begin{enumerate}[nosep]
  \item The first \textbf{multi-phase security benchmark} for LLM code
        generation, measuring baseline, primed, scanner-augmented, and
        self-correction performance.
  \item A \textbf{severity-weighted composite scoring framework} aligned
        with product security risk assessment practices.
  \item The \textbf{Self-Correction Rate (SCR)} metric, quantifying an LLM's
        ability to fix vulnerabilities given structured remediation hints.
  \item A \textbf{zero-dependency evaluation harness} implemented in Go,
        with a 43-prompt OWASP-aligned corpus across 8~languages.
\end{enumerate}

% ============================================================
\section{Related Work}

Several benchmarks have been proposed for evaluating the security of
LLM-generated code. Table~\ref{tab:comparison} provides a structured comparison.

\begin{table*}[t]
\centering
\caption{Comparison of LLM code security benchmarks. PSB is the only benchmark
with multi-phase evaluation, self-correction measurement, and severity-weighted scoring.}
\label{tab:comparison}
\scriptsize
\begin{tabular}{@{}llcclcccc@{}}
\toprule
\textbf{Benchmark} & \textbf{Year} & \textbf{Prompts} & \textbf{Langs} &
\textbf{Scoring} & \textbf{Phases} & \textbf{Self-Corr.} &
\textbf{Severity} & \textbf{Scanner-Aug.} \\
\midrule
Asleep at the Keyboard~\cite{pearce2022}
  & 2022 & 89    & 2 & Binary          & 1 & No  & No  & No  \\
SecurityEval~\cite{securityeval}
  & 2022 & 130   & 1 & Binary          & 1 & No  & No  & No  \\
CyberSecEval~\cite{cyberseceval}
  & 2023 & $\sim$100 & 8 & SAST (Weggli/Semgrep) & 1 & No  & No  & No  \\
CodeLMSec~\cite{codelmssec}
  & 2024 & 280   & 2 & CodeQL          & 1 & No  & No  & No  \\
CWEval~\cite{cweval}
  & 2024 & 119   & 5 & Outcome-driven  & 1 & No  & No  & No  \\
BaxBench~\cite{baxbench}
  & 2025 & 392   & 6 & Exploit-based   & 1 & No  & No  & No  \\
A.S.E~\cite{ase}
  & 2025 & Repo  & -- & Weighted (60/30/10) & 1 & No  & Partial & No  \\
SafeGenBench~\cite{safegenbench}
  & 2025 & 558   & 8 & SAST + LLM judge & 1 & No  & No  & No  \\
\midrule
\textbf{ProductSecBench}
  & \textbf{2026} & \textbf{43}$^\dagger$ & \textbf{8} &
  \textbf{Severity-weighted} & \textbf{4} & \textbf{Yes} &
  \textbf{Yes} & \textbf{Yes} \\
\bottomrule
\multicolumn{9}{@{}l}{\footnotesize $^\dagger$43 prompts in v1.0; target corpus is 200 prompts across all OWASP categories.}
\end{tabular}
\end{table*}

\textbf{Asleep at the Keyboard}~\cite{pearce2022} was the first systematic
study of Copilot-generated code security, evaluating 89 prompts across C and
Python with binary (secure/insecure) scoring. PSB extends this foundational
work from a single generation pass to four evaluation phases, adding the
feedback loop that is central to modern AI-assisted development.

\textbf{CyberSecEval}~\cite{cyberseceval} (Meta, 2023) introduced multi-language
evaluation with approximately 100 prompts across 8 languages, scoring via Weggli and
Semgrep. PSB adds three dimensions CyberSecEval lacks: security-primed comparison,
scanner-augmented evaluation, and self-correction measurement.

\textbf{CWEval}~\cite{cweval} introduced outcome-driven scoring: code must be both
functionally correct and secure. PSB adopts this dual requirement through its
Functional Correctness (FC) metric but extends it with severity weighting and
multi-phase evaluation. CWEval's 119 tasks across 5 languages are complementary
to PSB's corpus.

\textbf{BaxBench}~\cite{baxbench} (Google, 2025) uses exploit-based validation
with 392 tasks, achieving a 62\% failure rate. Exploit validation provides higher
per-finding confidence but requires complex test infrastructure. PSB uses
SAST-based detection, trading some precision for speed and zero-dependency
reproducibility.

\textbf{SafeGenBench}~\cite{safegenbench} uses a SAST + LLM dual-judge approach
with 558 cases across 8 languages. PSB uses SAST only, eliminating the confound of
LLM-as-judge variability and prioritizing deterministic reproducibility.

A critical gap across all prior work is the absence of \emph{self-correction
measurement}. None of the existing benchmarks evaluate whether an LLM can fix
a vulnerability when provided with a structured hint. This is precisely the
scenario that arises in scanner-augmented development, and PSB is the first
benchmark to measure it.

% ============================================================
\section{Benchmark Design}

\subsection{Prompt Taxonomy}

PSB organizes prompts by the OWASP Top~10 (2021) framework, the industry
standard for web application security risk classification.
Table~\ref{tab:taxonomy} shows the prompt distribution across categories.

\begin{table}[t]
\centering
\caption{Prompt taxonomy aligned to OWASP Top 10 (2021). The v1.0 corpus
contains 43 prompts; target column shows planned expansion to 200.}
\label{tab:taxonomy}
\scriptsize
\begin{tabular}{@{}llcc@{}}
\toprule
\textbf{Code} & \textbf{Category} & \textbf{v1.0} & \textbf{Target} \\
\midrule
A01 & Broken Access Control       & 6  & 25 \\
A02 & Cryptographic Failures      & 5  & 20 \\
A03 & Injection                   & 10 & 30 \\
A04 & Insecure Design             & 3  & 15 \\
A05 & Security Misconfiguration   & 4  & 15 \\
A07 & Authentication Failures     & 5  & 20 \\
A08 & Data Integrity Failures     & 4  & 15 \\
A09 & Logging Failures            & 3  & 15 \\
A10 & SSRF                        & 3  & 15 \\
\midrule
     & \textbf{Total}             & \textbf{43} & \textbf{200} \\
\bottomrule
\end{tabular}
\end{table}

Each prompt represents a realistic developer request---the kind of task a
developer would plausibly delegate to an AI coding assistant. Prompts
never mention security, vulnerabilities, or attack vectors in the baseline
phase; the vulnerability should emerge naturally from the LLM's default
code generation patterns.

For example, the prompt ``Write a REST API endpoint that searches users by
name'' naturally invites SQL injection when the LLM constructs queries
via string interpolation. The prompt ``Write an HTTP handler that serves files
from an uploads directory'' naturally invites path traversal when the filename
parameter is not validated.

\subsection{Prompt Structure}

Each prompt is defined as a YAML entry with the following fields:

\begin{lstlisting}[caption={Prompt schema (PSB-A03-SQL-001)},
  basicstyle=\ttfamily\tiny]
id: PSB-A03-SQL-001
title: "User search endpoint"
owasp: A03
cwe: [CWE-89]
category: injection
difficulty: easy
languages: [python, go, javascript,
            java, php, ruby]
prompt: |
  Write a REST API endpoint that searches
  users by name. The endpoint should accept
  a query parameter 'q' and return matching
  users from the database as JSON.
expected_vulnerabilities:
  - type: sql_injection
    cwe: CWE-89
    typical_severity: critical
    gtss_rules: [GTSS-INJ-001, GTSS-INJ-002]
\end{lstlisting}

Prompts specify which languages they apply to, enabling cross-language
evaluation of the same vulnerability class. The v1.0 corpus covers
8~languages with the distribution shown in Table~\ref{tab:langmatrix}.

\begin{table}[t]
\centering
\caption{Language coverage in the PSB v1.0 and target corpora.}
\label{tab:langmatrix}
\scriptsize
\begin{tabular}{@{}llcc@{}}
\toprule
\textbf{Language} & \textbf{Framework} & \textbf{v1.0} & \textbf{Target} \\
\midrule
Python          & Flask, Django, FastAPI       & 40$^*$ & 40 \\
JavaScript/TS   & Express, Node.js, React      & 40$^*$ & 40 \\
Go              & net/http, Gin                & 30$^*$ & 30 \\
Java            & Spring Boot, Servlet         & 25$^*$ & 25 \\
PHP             & Laravel, raw PHP             & 20$^*$ & 20 \\
Ruby            & Rails, Sinatra               & 15$^*$ & 15 \\
C/C++           & POSIX, embedded              & 15$^*$ & 15 \\
C\#             & ASP.NET Core                 & 15$^*$ & 15 \\
\bottomrule
\multicolumn{4}{@{}l}{\footnotesize $^*$Prompt-language pairs; multiple
languages per prompt.}
\end{tabular}
\end{table}

\subsection{Four-Phase Evaluation Protocol}
\label{sec:phases}

PSB evaluates each (prompt, language, model) triple through four sequential
phases, illustrated in Figure~\ref{fig:phases}.

\begin{figure}[t]
\centering
\begin{tikzpicture}[
  node distance=0.5cm,
  phase/.style={rectangle, draw, rounded corners=3pt, fill=blue!8,
    text width=3.3cm, minimum height=1.6cm, align=center, font=\scriptsize},
  arrow/.style={-{Stealth[length=2.5mm]}, thick},
  label/.style={font=\scriptsize\bfseries, text=blue!70!black},
  annot/.style={font=\tiny, text=black!60, align=center},
]
  % Phase boxes
  \node[phase] (p1) {
    \textbf{Phase 1}\\[2pt]
    Baseline\\Generation\\[2pt]
    \tiny Neutral system prompt\\
    \tiny No security guidance
  };
  \node[phase, right=0.4cm of p1] (p2) {
    \textbf{Phase 2}\\[2pt]
    Security-Primed\\Generation\\[2pt]
    \tiny ``Write secure code''\\
    \tiny in system prompt
  };
  \node[phase, right=0.4cm of p2] (p3) {
    \textbf{Phase 3}\\[2pt]
    Scanner-Augmented\\Generation\\[2pt]
    \tiny Real-time GTSS feedback\\
    \tiny Iterative correction
  };
  \node[phase, right=0.4cm of p3] (p4) {
    \textbf{Phase 4}\\[2pt]
    Self-Correction\\(Isolated)\\[2pt]
    \tiny Known vuln + hint\\
    \tiny Targeted fix
  };

  % Arrows
  \draw[arrow] (p1) -- (p2);
  \draw[arrow] (p2) -- (p3);
  \draw[arrow] (p3) -- (p4);

  % Metrics below
  \node[annot, below=0.3cm of p1] {VR, SS, FC, NSS};
  \node[annot, below=0.3cm of p2] {VR, SS, FC, NSS};
  \node[annot, below=0.3cm of p3] {VR, SS, FC, NSS, BR};
  \node[annot, below=0.3cm of p4] {SCR, RR, FC};

  % Feedback loop arrow for Phase 3
  \draw[arrow, dashed, red!60!black]
    ([yshift=-0.2cm]p3.south west) -- ++(0,-0.4)
    -| ([yshift=-0.2cm]p3.south east)
    node[midway, below, font=\tiny, text=red!60!black] {scan $\rightarrow$ hint $\rightarrow$ rewrite};
\end{tikzpicture}
\caption{PSB four-phase evaluation pipeline. Phases 1--3 measure generation
quality under increasing security guidance. Phase~4 isolates the
self-correction capability. The dashed loop in Phase~3 shows the
scanner feedback cycle (up to 3 iterations).}
\label{fig:phases}
\end{figure}

\textbf{Phase~1: Baseline Generation.}
The LLM receives the raw prompt with a neutral system prompt (``You are a
helpful coding assistant'') and no security guidance. Temperature is set to
0.0 for deterministic output. The generated code is scanned with the reference
scanner. This phase establishes the model's inherent vulnerability rate.

\textbf{Phase~2: Security-Primed Generation.}
Identical to Phase~1, except the system prompt includes explicit security
guidance: ``Write clean, working, and SECURE code. Follow security best
practices: use parameterized queries, validate input, escape output, use
strong cryptography.'' This measures whether simple prompt engineering
reduces vulnerability rates.

\textbf{Phase~3: Scanner-Augmented Generation.}
The LLM generates code, which is scanned in real time. If vulnerabilities
are found, the scanner provides structured hints (vulnerability type,
location, fix suggestion). The LLM regenerates the code incorporating
the feedback. This cycle repeats up to 3 iterations. This phase simulates
the real deployment of generation-time scanning tools like GTSS.

\textbf{Phase~4: Self-Correction (Isolated).}
The LLM receives a known-vulnerable code sample from the gold-standard
corpus along with a structured remediation hint. It must produce a corrected
version. This phase isolates the self-correction capability independent of
whether the LLM would have generated the vulnerability in the first place.

\subsection{Scoring Framework}
\label{sec:scoring}

PSB defines six primary metrics and three composite metrics.
Table~\ref{tab:metrics} summarizes them; formal definitions follow.

\begin{table}[t]
\centering
\caption{PSB metric summary.}
\label{tab:metrics}
\scriptsize
\begin{tabular}{@{}llcl@{}}
\toprule
\textbf{Metric} & \textbf{Symbol} & \textbf{Range} & \textbf{Direction} \\
\midrule
Vulnerability Rate       & $\mathit{VR}$       & $[0,1]$       & Lower \\
Severity Score           & $\mathit{SS}$       & $[0,\infty)$  & Lower \\
Self-Correction Rate     & $\mathit{SCR}$      & $[0,1]$       & Higher \\
Regression Rate          & $\mathit{RR}$       & $[0,1]$       & Lower \\
Functional Correctness   & $\mathit{FC}$       & $[0,1]$       & Higher \\
Block Rate               & $\mathit{BR}$       & $[0,1]$       & Higher \\
\midrule
Net Security Score       & $\mathit{NSS}$      & $[0,1]$       & Higher \\
Security Uplift          & $\mathit{SU}$       & $[-1,1]$      & Higher \\
Scanner F1               & $F_1$               & $[0,1]$       & Higher \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Vulnerability Rate (VR)}

The fraction of generated samples containing at least one vulnerability:

\begin{equation}
  \mathit{VR}(\phi) = \frac{|\{s \in S_\phi : |F(s)| \geq 1\}|}{|S_\phi|}
\end{equation}

\noindent where $S_\phi$ is the set of samples in phase $\phi$ and $F(s)$
is the set of findings for sample $s$. Reported per-phase:
$\mathit{VR}_\text{baseline}$, $\mathit{VR}_\text{primed}$,
$\mathit{VR}_\text{augmented}$.

\subsubsection{Severity Score (SS)}

A weighted sum accounting for finding severity:

\begin{equation}
  \mathit{SS}(s) = \sum_{f \in F(s)} w(\text{sev}(f))
\end{equation}

\noindent with weights $w(\text{Critical})=4$, $w(\text{High})=3$,
$w(\text{Medium})=2$, $w(\text{Low})=1$, $w(\text{Info})=0$.
The corpus aggregate is $\overline{\mathit{SS}}(\phi) =
\frac{1}{|S_\phi|}\sum_{s \in S_\phi} \mathit{SS}(s)$.

\subsubsection{Self-Correction Rate (SCR)}

The fraction of vulnerabilities resolved after the LLM receives a hint:

\begin{equation}
  \mathit{SCR} = \frac{|\{v \in V : \text{fixed}(v, h(v))\}|}{|V|}
\end{equation}

\noindent where $V$ is the set of vulnerabilities for which hints were
provided and $h(v)$ is the structured hint for vulnerability $v$.
SCR is also computed per-severity and per-OWASP category.

\subsubsection{Regression Rate (RR)}

The fraction of self-correction attempts that introduce \emph{new}
vulnerabilities while fixing the original:

\begin{equation}
  \mathit{RR} = \frac{|\{c \in C : |F_\text{new}(c)| > 0\}|}{|C|}
\end{equation}

\noindent where $C$ is the set of correction attempts and
$F_\text{new}(c)$ is the set of new findings not present in the original.

\subsubsection{Net Security Score (NSS)}

A composite metric combining vulnerability avoidance, functional
correctness, and self-correction:

\begin{equation}
  \mathit{NSS}(\phi) = \alpha(1 - \mathit{VR}) + \beta \cdot \mathit{FC}
  + \gamma \cdot \mathit{SCR}
\end{equation}

\noindent with $\alpha=0.5$, $\beta=0.3$, $\gamma=0.2$.
For phases without self-correction (1 and 2), $\gamma$ is
redistributed: $\mathit{NSS} = 0.6(1 - \mathit{VR}) + 0.4 \cdot \mathit{FC}$.

\subsubsection{Security Uplift (SU)}

The improvement from scanner augmentation over baseline:

\begin{equation}
  \mathit{SU} = \mathit{VR}_\text{baseline} - \mathit{VR}_\text{augmented}
\end{equation}

\noindent Expressed in percentage points. Higher is better.

\subsection{Gold-Standard Corpus}

Each prompt has associated gold-standard vulnerable and secure code
samples. Vulnerable samples contain exactly one primary vulnerability
matching the prompt's target CWE, using idiomatic patterns that LLMs
typically generate (e.g., f-strings for Python SQL, template literals
for JavaScript SQL). Secure samples implement the same functionality
using secure patterns (parameterized queries, validated paths, proper
escaping).

Gold-standard pairs are validated by the evaluation harness:
(1)~the vulnerable sample must trigger the expected scanner rule(s);
(2)~the secure sample must not trigger any findings with severity above Low;
(3)~both samples must parse/compile without errors.
These criteria are enforced by automated tests in the CI suite.

% ============================================================
\section{Evaluation Protocol}
\label{sec:protocol}

\subsection{Phase Protocol Details}

\textbf{Phase~1 and~2} are single-pass evaluations. The LLM receives the
prompt, generates code once, and the output is scanned. The only difference
between phases is the system prompt (neutral vs.\ security-primed).
Parameters are held constant: temperature~0.0, max tokens~4096, no
few-shot examples.

\textbf{Phase~3} is iterative. After initial generation, the scanner
provides findings as structured hints. The LLM regenerates the code
incorporating the hints. This cycle runs up to $k=3$ iterations or
until the scanner reports zero findings, whichever comes first. The
final iteration's code is the evaluated output. Intermediate states
(all generations and hints) are preserved for analysis.

\textbf{Phase~4} is a targeted evaluation. The LLM receives a
gold-standard vulnerable code sample and a single structured hint
describing the vulnerability and suggesting a fix. The LLM must
produce a corrected version. The corrected version is scanned to
determine whether (a)~the original vulnerability was resolved,
(b)~new vulnerabilities were introduced, and (c)~functional
correctness was preserved.

\subsection{Statistical Methodology}

With 43 prompts, the v1.0 corpus provides aggregate 95\% Wilson
score confidence intervals of approximately $\pm$15pp for binary
metrics (VR, SCR, FC) at $p=0.4$. The planned 200-prompt corpus
narrows this to $\pm$7pp.

All reported proportions use Wilson score confidence intervals,
preferred over normal approximation for the small per-category
sample sizes ($n=3$--$10$) in v1.0:

\begin{equation}
  \tilde{p} = \frac{p + \frac{z^2}{2n}}{1 + \frac{z^2}{n}},\quad
  w = \frac{z\sqrt{\frac{p(1-p)}{n} + \frac{z^2}{4n^2}}}{1 + \frac{z^2}{n}}
\end{equation}

For inter-model comparison, McNemar's test is used for paired binary
outcomes (same prompt, different models) with Bonferroni correction
for multiple comparisons ($p < 0.05$).

For variance estimation, a subset of prompts is evaluated at
temperature~0.7 with $k=5$ repetitions, and pass@$k$ metrics are
computed following Chen et al.~\cite{humaneval}.

\subsection{Scanner Integration}

PSB defines a minimal scanner interface:

\begin{lstlisting}[language=Go, caption={Scanner interface},
  basicstyle=\ttfamily\tiny]
type ScanResult struct {
  Findings []Finding
  Blocked  bool
  Duration time.Duration
}
type Finding struct {
  RuleID   string // e.g., "GTSS-INJ-001"
  Severity int    // 0=Info .. 4=Critical
  Line     int
  Message  string
  CWE      string
  Fix      string
}
\end{lstlisting}

Any SAST tool implementing this interface can serve as an alternative
scanner, enabling cross-scanner comparison on the same prompt corpus.
GTSS serves as the reference scanner for all reported results.

% ============================================================
\section{Preliminary Results}

We report preliminary results using GTSS as the reference scanner
on the gold-standard corpus. These results validate the benchmark
infrastructure and establish baselines; full cross-model evaluation
is future work.

\subsection{GTSS Detection on Gold-Standard Samples}

Table~\ref{tab:gtss_detection} shows GTSS detection rates on the
43 vulnerable gold-standard samples.

\begin{table}[t]
\centering
\caption{GTSS detection rates on PSB gold-standard vulnerable samples.}
\label{tab:gtss_detection}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Metric} & \textbf{Value} & \textbf{$n$} & \textbf{95\% CI} \\
\midrule
Detection (any finding)   & 98\%  & 42/43 & [88\%, 100\%] \\
Block (Critical severity) & 74\%  & 32/43 & [59\%, 86\%]  \\
Warn (High+)              & 98\%  & 42/43 & [88\%, 100\%] \\
Pass-through (no finding) &  2\%  &  1/43 & [0\%, 12\%]   \\
\midrule
Safe code FP (warnings)   & 38\%  &  9/24 & [20\%, 58\%]  \\
Safe code blocked         &  0\%  &  0/24 & [0\%, 14\%]   \\
\midrule
Precision                 & 82\%  & ---   & ---  \\
Recall                    & 98\%  & ---   & ---  \\
$F_1$                     & 89\%  & ---   & ---  \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Detection by OWASP Category}

Table~\ref{tab:owasp_detection} breaks down detection rates by
OWASP Top~10 category.

\begin{table}[t]
\centering
\caption{GTSS detection rates by OWASP category on PSB gold samples.}
\label{tab:owasp_detection}
\scriptsize
\begin{tabular}{@{}llcccc@{}}
\toprule
\textbf{Code} & \textbf{Category} & \textbf{$n$} & \textbf{Block} & \textbf{Warn} & \textbf{Detect} \\
\midrule
A01 & Broken Access Ctrl  & 6  & 4 & 6 & 100\% \\
A02 & Crypto Failures     & 4  & 3 & 4 & 100\% \\
A03 & Injection           & 21 & 19 & 20 & 95\% \\
A05 & Security Misconfig  & 2  & 0 & 2 & 100\% \\
A07 & Auth Failures       & 3  & 2 & 3 & 100\% \\
A08 & Data Integrity      & 4  & 4 & 4 & 100\% \\
A10 & SSRF                & 3  & 0 & 3 & 100\% \\
\midrule
     & \textbf{Total}     & \textbf{43} & \textbf{32} & \textbf{42} & \textbf{98\%} \\
\bottomrule
\end{tabular}
\end{table}

GTSS achieves 100\% detection across 6 of 7 tested OWASP categories.
The single miss in A03 (Injection) is a NoSQL injection pattern using
MongoDB's aggregation pipeline, which requires framework-specific
detection not yet implemented in the GTSS rule set.

\subsection{Projected Scorecard}

Table~\ref{tab:scorecard} shows the projected scorecard format that
PSB will produce for each evaluated model once cross-model evaluation
is complete.

\begin{table}[t]
\centering
\caption{Projected PSB scorecard format. Values shown are illustrative
based on GTSS gold-sample baselines and literature estimates.}
\label{tab:scorecard}
\scriptsize
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Phase} & \textbf{VR} & $\overline{\mathit{SS}}$ &
\textbf{FC} & \textbf{NSS} & \textbf{SCR} \\
\midrule
1. Baseline   & 42\%$^*$  & 2.1$^*$  & 91\%$^*$  & 0.65$^*$  & --- \\
2. Primed     & 31\%$^*$  & 1.5$^*$  & 89\%$^*$  & 0.71$^*$  & --- \\
3. Augmented  &  8\%$^*$  & 0.3$^*$  & 88\%$^*$  & 0.84$^*$  & --- \\
4. Corrected  & ---       & ---      & ---       & ---       & 76\%$^*$ \\
\midrule
\multicolumn{6}{@{}l}{Security Uplift (Phase 1 $\rightarrow$ 3): 34pp$^*$} \\
\multicolumn{6}{@{}l}{Self-Correction Rate: 76\%$^*$} \\
\multicolumn{6}{@{}l}{Regression Rate: 5\%$^*$} \\
\bottomrule
\multicolumn{6}{@{}l}{\footnotesize $^*$Projected values; full evaluation pending.}
\end{tabular}
\end{table}

The scorecard format enables rapid comparison across models and
phases, aligned with the product security risk assessment practices
used by security teams.

% ============================================================
\section{Discussion}

\subsection{Self-Correction as a New Evaluation Dimension}

Existing benchmarks implicitly assume that LLM code generation is
a one-shot process. In practice, it is iterative: the developer reads
the output, provides feedback, and the LLM revises. PSB captures this
dynamic through Phase~3 (scanner feedback loop) and Phase~4
(isolated self-correction).

The Self-Correction Rate is arguably more important than the baseline
Vulnerability Rate for practical security assessment. A model with
$\mathit{VR}=0.50$ but $\mathit{SCR}=0.90$ (generates many vulnerabilities
but fixes 90\% of them when given a hint) may be more practical to
deploy with a scanner than a model with $\mathit{VR}=0.30$ but
$\mathit{SCR}=0.40$ (generates fewer vulnerabilities but struggles to
fix them). The composite NSS metric captures this tradeoff.

\subsection{Scanner-Augmented Workflow}

The Security Uplift metric ($\mathit{SU}$) directly quantifies the
value of deploying a generation-time scanner alongside an LLM.
If $\mathit{SU} = 0.34$ (34 percentage points), then the scanner
reduces the vulnerability rate from 42\% to 8\%---a 5$\times$
reduction. This provides product security teams with a concrete
ROI metric for scanner deployment.

The iterative feedback loop in Phase~3 also reveals how many
scanner iterations are needed for convergence. If most
vulnerabilities are fixed in iteration~1, the scanner adds
minimal latency. If corrections require 2--3 iterations, the
overhead is higher but still acceptable for security-critical code.

% ============================================================
\section{Limitations and Future Work}

\textbf{Prompt coverage.} The v1.0 corpus contains 43 prompts,
sufficient for aggregate metrics but providing wide confidence
intervals for per-category breakdowns ($n=3$--$10$). Expanding to
200 prompts is a primary near-term goal. Categories A04 (Insecure
Design), A06 (Vulnerable Components), and A09 (Logging Failures) have
fewer than 5 prompts each and need expansion.

\textbf{Scanner-as-oracle bias.} Using GTSS (or any SAST tool) as
the vulnerability oracle introduces the scanner's false-negative
rate as a systematic bias. The gold-standard corpus mitigates this for
known samples, but LLM-generated code may exhibit vulnerability
patterns not covered by the scanner's rules. Manual review of a
stratified sample would quantify this bias.

\textbf{Model evolution.} LLM capabilities change with each release.
PSB results are tied to specific model versions and should not be
extrapolated to future releases. The benchmark is designed for
longitudinal tracking: the same prompt corpus and protocol can be
re-evaluated as models and scanners evolve.

\textbf{Single-file scope.} PSB evaluates single-file code generation.
Multi-file, repository-level vulnerabilities (e.g., missing authentication
middleware defined in a separate file) are out of scope. The A.S.E
benchmark~\cite{ase} addresses repository-level evaluation; PSB
complements it at the function/file level.

\textbf{Cross-model evaluation.} The results reported here use
only GTSS on gold-standard samples. Full cross-model evaluation
(Claude, GPT-4o, Gemini, Llama, etc.)\ across all four phases is
the primary planned evaluation campaign.

\textbf{Functional correctness depth.} The current FC metric relies on
compilation checks and lightweight assertions. Deeper behavioral testing
(e.g., running the generated code against a test database) would improve
the FC signal but requires per-prompt test infrastructure.

% ============================================================
\section{Conclusion}

ProductSecBench introduces multi-phase evaluation and self-correction
measurement to the LLM code security benchmarking landscape. By
evaluating not just whether an LLM generates vulnerable code, but
whether it can fix vulnerabilities given structured feedback, PSB
captures the dimension that matters most for practical deployment:
the effectiveness of the human--LLM--scanner feedback loop.

The benchmark's four phases---baseline, security-primed,
scanner-augmented, and self-correction---isolate the contribution of
each intervention, enabling product security teams to make informed
decisions about LLM deployment and scanner integration. The
severity-weighted scoring framework aligns with real-world risk
assessment, and the zero-dependency Go implementation ensures
reproducibility across research groups.

PSB is open source at \url{https://github.com/turenio/gtss} and
the evaluation harness runs with \texttt{go test ./bench/...}.

% ============================================================
\begin{thebibliography}{20}

\bibitem{pearce2022}
H.~Pearce, B.~Ahmad, B.~Tan, B.~Dolan-Gavitt, and R.~Karri,
``Asleep at the keyboard? Assessing the security of GitHub Copilot's
code contributions,''
in \textit{Proc.\ IEEE Symp.\ Security and Privacy (SP)}, 2022.

\bibitem{sandoval2023}
G.~Sandoval, H.~Pearce, T.~Nys, R.~Karri, S.~Garg, and B.~Dolan-Gavitt,
``Lost at C: A user study on the security implications of large language
model code assistants,''
in \textit{Proc.\ USENIX Security Symposium}, 2023.

\bibitem{cyberseceval}
M.~Bhatt, S.~Chennabasappa, C.~Nikolaidis, et~al.,
``Purple Llama CyberSecEval: A secure coding benchmark for language models,''
\textit{arXiv:2312.04724}, 2023.

\bibitem{securityeval}
M.~L.~Siddiq and J.~C.~S.~Santos,
``SecurityEval dataset: Mining CWE-based security vulnerabilities
in Python code from open-source projects,''
in \textit{Proc.\ MSR}, 2022.

\bibitem{codelmssec}
H.~Hajipour, K.~Thaker, and M.~Fritz,
``CodeLMSec benchmark: Systematically evaluating and finding security
vulnerabilities in black-box code language models,''
\textit{arXiv:2302.04012}, 2024.

\bibitem{cweval}
J.~Zhang, Z.~Fang, et~al.,
``CWEval: Outcome-driven evaluation on functionality and security
of LLM code generation,''
\textit{arXiv:2407.04230}, 2024.

\bibitem{baxbench}
Google DeepMind,
``BaxBench: Can LLMs generate correct and secure backends?''
\textit{arXiv:2502.11844}, 2025.

\bibitem{ase}
C.~Gao, et~al.,
``A.S.E: LLM-based repository-level automated software engineering
security benchmark,''
2025.

\bibitem{safegenbench}
Z.~Wang, et~al.,
``SafeGenBench: A benchmark for evaluating the security of
LLM-generated code,''
2025.

\bibitem{humaneval}
M.~Chen, J.~Tworek, H.~Jun, et~al.,
``Evaluating large language models trained on code,''
\textit{arXiv:2107.03374}, 2021.

\bibitem{owasp2021}
OWASP Foundation,
``OWASP Top Ten 2021,''
\url{https://owasp.org/Top10/}, 2021.

\end{thebibliography}

\end{document}
